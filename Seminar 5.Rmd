---
title: "Seminar 5"
author: "Lydwin Wagenaar"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Seminar 5
#Binomial and poission distributions
```{r}
#rbinom(3, 10, c(0.1, 0.5, 0.9))
x = seq(from=0, to=1, by=0.01)
v_b = x*(1-x) #Binomial variance
plot(x, v_b, type="l", xlab="Probability", ylab="Theoretical variance", las=1)
```
```{r}
logit = function(x) log(x/(1-x))
invlogit = function(x) 1/(1+exp(-x))
x = runif(200)
logit_x = logit(x)
par(mfrow=c(2,2))
hist(x, las=1)
hist(logit_x, las=1)
xx = seq(-5, 5, 0.01)
plot(xx, invlogit(xx), type="l", las=1,
xlab="Logit (x)",
ylab="P")
plot(x, invlogit(logit_x), las=1)
```
```{r}
plot(xx, invlogit(xx), type="l", las=1,
xlab="Logit/Probit (x)",
ylab="P")
lines(xx, pnorm(xx), lty=2)
legend("topleft", legend=c("Logit", "Probit"),
lty=c(1,2))

```
#Logistic regression
```{r}
x = rnorm(200, 10, 3)
eta = -2 + 0.4*x + rnorm(200, 0, 2)
p = invlogit(eta)
y = rbinom(200, 1, p)
par(mfrow=c(1,3))
plot(x, eta, las=1)
plot(x, p, las=1)
plot(x, y, las=1)
```
```{r}
m = glm(y~x, family=binomial(link="logit"))
summary(m)

```
```{r}
coefs = summary(m)$coef
x_pred = seq(from=min(x), to=max(x), by=0.01)
y_hat = coefs[1,1] + coefs[2,1]*x_pred
p_hat = invlogit(y_hat)
plot(x,y,las=1) 
lines(x_pred,p_hat)
abline(v=-coefs[1,1]/coefs[2,1], col="blue",lty=2) #v = yhat = 0 because invlogit(0) = 0.5
abline(h=0.5, col="blue",lty=2)

```
The GLM summary table does not provide an r
2 value, because the normal r
2 does not work for logistic
regression. There are however several ‘Pseudo-r 2’ available, typically based on comparing the likelihood of
the model to that of a null model (a similar model but with only an intercept). The MuMIn package provides
one such measure.
```{r}
library(MuMIn)
r.squaredGLMM(m)
```
or Tjur’s D
```{r}
y_hat = coefs[1,1] + coefs[2,1]*x
p_hat = invlogit(y_hat)
mean(p_hat[which(y==1)]) - mean(p_hat[which(y==0)])
```
Some final notes on fitting binomial GLM’s. There are three ways to formulate these models in R. In the
example above, the data were 0’s and 1’s, and we could specify the model simply as
```{r}
glm(y ~ x, family=binomial(link="logit"))
```
When each observation is based on more than one trial, we can formulate the model in two ways. The first is
```{r}
glm(y ~ x, family=binomial(link="logit"), weights=n)
```

where y is the proportion of successes, and n is the number of trials. The second method is to fit a two-column
matrix as response variable, where the first colomn is the number of successes, and the second column is the
number of failures, i.e. y = cbind(successes, failures). The model formula is then
```{r}
glm(cbind(successes, failures) ~ x, family=binomial(link="logit"))

```

##Data exercise: seed germination

```{r}
Dorm <- read.csv("C:/Users/ly5276wa/Work Folders/Desktop/phd/Courses/Statistics course/R/CourseStat/dormancy.csv")
```

Analyse the data to estimate the pattern of germination success in response to variation in the duration of after-ripening. 

Are the patterns similar in different populations? 

Are there other factors affecting germination success? 

Produce relevant summary statistics, parameter estimates, and graphs.

```{r}
#first explore the data
hist(Dorm$timetosowing)
hist(Dorm$MCseed)

subdat = Dorm[Dorm$pop=="CC",]
germ = subdat$germ2 * subdat$nseed #Successes
notgerm = subdat$nseed - germ #Failures
mod1 = glm(cbind(germ, notgerm) ~ timetosowing, "binomial", data=subdat)
mod2 = glm(germ2 ~ timetosowing, "binomial", weights=nseed, data=subdat)
logLik(mod1) == logLik(mod2)

#Analyse the data to estimate the pattern of germination success in response to variation in the duration of after-ripening. 
#total dataset
m <- glm(germ2 ~ timetosowing, "binomial",weights = nseed, data = Dorm)
summary(m)
#with 1 day increase in time to sowing, the log odds of germination increased with 0.03 (SE = +- 0.002). 
#coefs = summary(m)$coef
#y_hat = coefs[1,1] + coefs[2,1]*Dorm$timetosowing
#p_hat = invlogit(y_hat)
#mean(p_hat[which(y==1)]) - mean(p_hat[which(y==0)]) #not with proportions!
r.squaredGLMM(m) #delta one is more flexible


```
43 % can be explained by the model


Are the patterns similar in different populations? 

```{r}
#lets add populations to the model.
mp <- glm(germ2 ~ timetosowing * pop, "binomial",weights = nseed, data = Dorm)
mp$xlevels
summary(mp)
```
Since the populations are so different, which is not really the point of this study, we will look at the data per population instead. We could leave it in the model but the interactions gives it quite some complicity. 

```{r}
subdat = Dorm[Dorm$pop=="CC",]
mod2 = glm(germ2 ~ timetosowing, "binomial", weights=nseed, data=subdat)
summary(mod2)
#lets see if there are other factors affecting the results.
mod3 = glm(germ2 ~ timetosowing * MCseed + mother, "binomial", weights=nseed, data=subdat)
summary(mod3)
boxplot(subdat$MCseed~subdat$mother)
#mother should be a random variable but we dont do that yet so lets leave it out for now.
mod4 = glm(germ2 ~ timetosowing * MCseed, "binomial", weights=nseed, data=subdat)
summary(mod4)
```
Lets make a figure without interaction because the interaction is only explaining very little of the data
```{r}
summary(mod4)
coefs <- summary(mod4)$coef
#interaction is positive so we can say that heavier seeds increase the time affecting the germination.
coefs
x_pred = seq(from=min(subdat$timetosowing, na.rm=T), to=max(subdat$timetosowing, na.rm=T), by=0.01) #making the probability graph
y_hat = coefs[1,1] + coefs[2,1]*x_pred
p_hat = invlogit(y_hat)
y_hat2 = coefs[1,1] + coefs[2,1]*x_pred + coefs[3,1]*sd(subdat$MCseed)
y_hat3 = coefs[1,1] + coefs[2,1]*x_pred - coefs[3,1]*sd(subdat$MCseed)

#without interaction
mod5 = glm(germ2 ~ timetosowing + MCseed, "binomial", weights=nseed, data=subdat)
coefs1 <- summary(mod5)$coef
x_pred = seq(from=min(subdat$timetosowing, na.rm=T), to=max(subdat$timetosowing, na.rm=T), by=0.01) #making the probability graph
y_hat1 = coefs1[1,1] + coefs1[2,1]*x_pred + coefs1[3,1]
p_hat1 = invlogit(y_hat1)
y_hat4 = coefs1[1,1] + coefs1[2,1]*x_pred + coefs1[3,1]*sd(subdat$MCseed) #one seed + sd and one -
y_hat5 = coefs1[1,1] + coefs1[2,1]*x_pred - coefs1[3,1]*sd(subdat$MCseed)

plot(subdat$timetosowing,subdat$germ2,las=1) 
lines(x_pred,p_hat)
abline(v=-coefs[1,1]/coefs[2,1], col="blue",lty=2)
abline(v=-coefs1[1,1]/coefs1[2,1], col="red",lty=2)#v = yhat = 0 because invlogit(0) = 0.5
abline(h=0.5, col="purple",lty=2)
lines(x_pred,p_hat1, col = "red")
lines(x_pred, invlogit(y_hat2), lty=2)
lines(x_pred, invlogit(y_hat3), lty=2)
lines(x_pred, invlogit(y_hat4), lty=2, col="red")
lines(x_pred, invlogit(y_hat5), lty=2, col="red")
#with heavier seeds it needs more days before germination probability is 0.5% which was seen in the summary table since interaction was a + number

v <- -coefs1[1,1]/coefs1[2,1]
v
```
To calculate the duration of after-ripening needed for a 50% germination rate, we use the equation above to find that this would be 106.7 days in this population.


To quantify the seed size effect, we can ask how this changes for a seed that is one standard deviation larger or smaller than the mean.

```{r}
-(coefs[1,1] + coefs[3,1]*sd(subdat$MCseed))/coefs[2,1] #(intercept + MCseed*sd) / slope time to sowing.
-(coefs[1,1] - coefs[3,1]*sd(subdat$MCseed))/coefs[2,1]

-(coefs1[1,1] + coefs1[3,1]*sd(subdat$MCseed))/coefs1[2,1] #(intercept + MCseed*sd) / slope time to sowing.
-(coefs1[1,1] - coefs1[3,1]*sd(subdat$MCseed))/coefs1[2,1]
```

We could write the results like this: The probability of germination increased with longer duration of after-ripening (Fig. 1, Table 1). A seed of average size would have 50% probability of germinating when sown after 106.7 days of after-ripening. However, there is a small interaction with seed size that shows that for heavier seeds, this is 109 days instead. For a seed one standard deviation larger or smaller than the mean, this period would change to 129.4 days and 84.0 days, respectively. But with the interaction this is 142.2507 and 76.54 days instead.

## Part 2: Poisson and negative-binomial regression

For count data, the data distribution is often skewed, and the variance tends to increase
with the mean. The Poisson distribution is tailored for such data. The variance increases linearly with the mean.

```{r}
x = rpois(200, 3)
hist(x, las=1)
```
 The probability mass function
changes quite dramatically for different values of λ. For low values the distribution is highly skewed, for
high values the distribution approaches a Gaussian (normal) distribution. However, the variance is still
constrained to be the same as the mean, which is not the case for the normal distribution.

```{r}
x = seq(0, 20, 1)
y = dpois(x, lambda=1)
plot(x,y, type="b", las=1, xlab="k", ylab="P(x=k)", pch=16, col=1)
points(x, dpois(x, lambda=3), type="b", pch=16, col=2)
points(x, dpois(x, lambda=10), type="b", pch=16, col=3)
legend("topright", col=1:3, pch=16,
legend=c(expression(paste(lambda, " = 1")),
expression(paste(lambda, " = 3")),
expression(paste(lambda, " = 10"))))
```

The distribution of count data can sometimes by normalized through a log-transformation, and the log is
indeed the link function of a Poisson regression model. The alternative method of log-transforming the
data and then fitting a Gaussian model is problematic when there are zeros in the data. Adding a constant
(e.g. 0.5 or 1) is sometimes an option, but is generally not recommended. A better option is to analyze the
data in a GLM framework with Poisson-distributed errors and a log link function.

```{r}
x = rnorm(200, 10, 3)
eta = -2 + 0.2*x
y = ceiling(exp(eta + rpois(200, 0.3)))
par(mfrow=c(1,2))
plot(x, eta, las=1)
plot(x, y, las=1)
```
```{r}
m = glm(y~x, family="poisson")
summary(m)
```
When interpreting Poisson regressions, there are several things to keep in mind. First, as in all GLMs,
the parameters are reported on the link scale, here log. Recall that the link scale has useful proportional
properties, so that the slope can be interpreted roughly as the proportional change in y per unit change in
x. To plot the fitted regression line, we have to back-transform the predicted values. This time we use the
generic predict function to obtain the predicted values on the data scale, and to construct a 95% confidence
polygon.

```{r}
plot(x, y, las=1, col="darkgrey", pch=16)
xx = seq(min(x), max(x), 0.01)
y_hat = predict(m, newdata=list(x=xx), type="response", se.fit=T)
lines(xx, y_hat$fit)
#lines(xx, y_hat$fit+1.96*y_hat$se.fit, lty=2)
#lines(xx, y_hat$fit-1.96*y_hat$se.fit, lty=2)
polygon(c(xx, rev(xx)),
c(y_hat$fit+1.96*y_hat$se.fit,
rev(y_hat$fit-1.96*y_hat$se.fit)),
col = rgb(0,1,0,.5), border = FALSE)

```

As in logistic regression the normal r2
is not valid, and we can use e.g. the r.squaredGLMM function to obtain
a Pseudo r2.

```{r}
library("MuMIn")
r.squaredGLMM(m)
#or, but old method
1-(m$deviance/m$null.deviance)

```
The Pseudo r2 above is based on comparing the (mis)fit of the focal model to a null model with
only an intercept. If a model is as bad as a null model, the Pseudo r
2 will be 0. We can also use the deviance
to compare other models, e.g. through likelihood ratio tests (more on that later).

Second, recall that the Poisson distribution has a single parameter λ determining both the mean and the variance. In real count data the variance often increase disproportionally compared to the mean, a phenomenon
called overdispersion. Biologically, this occurs because we tend to observe multiple entities together. For
example, in a survey of common eiders, we will often see either no eiders, or a lot of eiders.
We can quantify overdispersion in the data based on the fitted model by calculating the ratio of the residual
deviance to the residual degrees of freedom. In the model above there is no serious overdispersion, because
the residual deviance is only a little larger than the residual degrees of freedom. Let us construct an example
with more serious overdispersion.

```{r}
set.seed(1)
x = rnorm(200, 10, 3)
eta = -2 + 0.2*x
y = floor(exp(eta + rnbinom(200, 1, mu=.8)))
par(mfrow=c(1,2))
plot(x, eta, las=1)
plot(x, y, las=1)
```
```{r}
m = glm(y~x, family="poisson")
summary(m)

```
Here the overdispersion is serious, and we can not trust the model estimates. In this case, we need an
alternative link function that allows the variance to increase more than the mean. The negative binomial
distribution is a good option. The negative binomial is similar to the Poisson distribution, but includes an
additional parameter modelling the disproportionate increase in variance with increasing mean.

```{r}
library(MASS)
m = glm.nb(y~x)
summary(m)
```
## Data exercise:  Bee distribution

```{r}
Bee <- read.csv("C:/Users/ly5276wa/Work Folders/Desktop/phd/Courses/Statistics course/R/CourseStat/Eulaema.csv")

```
Use a GLM to build a model
explaining the distribution patterns of Eulaema nigrita. Interpret the results and produce nice tables and
figures.
Effort: log(hours fieldwork)

We are working with count data (abundance of bees) so we have to maybe use the poisson distribution or negative binomial distribution.

RQ: How is the bee abundance dependent on land use heterogeneity?

Lets first explore the data:
The bees have been samples with different methods
```{r}
hist(Bee$Eulaema_nigrita) #many have small counts and then sometimes they found a lot of bees
hist(Bee$effort) #different efforts to sample the bees.
#best to divide the effort with the number of bees as a way to weight the sampling effort
Bee$Abundance <- Bee$Eulaema_nigrita / Bee$effort #only if the slope is 1, otherwise you get something weird. Otherwise put it as a covariate.
hist(Bee$Abundance)
boxplot(Eulaema_nigrita~method,data = Bee)
#It does seem like the method has a little effect.

range(Bee$Eulaema_nigrita)
```
They have used different methods to sample the bees so that should be a variable in the model as well.  


```{r}
#lets look at the environmental variables together to also check for colinearity
library("psych")
pairs.panels(Bee[c(3:8)],)
```
It seems that mostly altitude is correlated to temperature, but unsure if it is too bad. We can put it in the model and check the VIF values to see if it is too correlated to keep it in.

```{r}
pairs.panels(Bee[c(9:10)],)
#connected but not linearly. Maybe look at only one of them because it makes sense that it is connected. Only use heterogeneity since that is what I want to study.
```
Does not seem to be very large outliers either. 

Enough explored, lets fit a model!

```{r}
model = glm(Bee$Eulaema_nigrita ~ Bee$lu_het + Bee$method + Bee$effort + Bee$MAT + Bee$altitude + Bee$MAP, family="poisson")
library("car")
vif(model)
#like I thought, collinearity between altitude and temperature. We keep temperature in it because that explains likely a bit more.
model1 = glm(Bee$Eulaema_nigrita ~ Bee$lu_het + Bee$method + Bee$effort + Bee$MAT + Bee$MAP, family="poisson")
vif(model1)
#no problems with collinearity in this model anymore.
summary(model1)
#but we definitely see overdispersion, so we switch to a negative binomial distribution.
library(MASS)
model3 = glm.nb(Bee$Eulaema_nigrita ~ Bee$lu_het + Bee$method + Bee$effort + Bee$MAT + Bee$MAP, na = na.exclude)
anova(model3)
summary(model3)
#as we expected from the boxplot, the method did not include that much 
r.squaredGLMM(model3)
#lets leave method out of it to simplify the model and method did not show any importance
model3 = glm.nb(Bee$Eulaema_nigrita ~ Bee$lu_het + Bee$effort + Bee$MAT + Bee$MAP, na = na.exclude)
anova(model3)
summary(model3)
range(Bee$MAT)
#with the model having a very small estimate and a bigger standard error, we cannot show any effect of it on the bee abundance. If we look at the range of temperature it goes from 135 and 263 temperature which would explain 5% (0.0005 * 100 from temperature and then log scale so 5%) of the bee abundance which is not a lot so we can exclude this variable.
#(estimate^2 * variance predictor) / total variance #to calculate the partial variance of variables, difficult with complicated models.

model3.5 = glm.nb(Bee$Eulaema_nigrita ~ Bee$lu_het + Bee$effort + Bee$method + Bee$MAP, na = na.exclude)
anova(model3.5)
summary(model3.5)
range(Bee$MAT)

r.squaredGLMM(model3)

# FINAL MODEL:
model4 = glm.nb(Eulaema_nigrita ~ lu_het + effort + MAP,data = Bee, na = na.exclude)
anova(model4)
summary(model4)
range(Bee$MAP) #map has very large range so small estimate does explain something: 2400 difference --> within this whole change 360 % change in bee population explained (0.15% per MAP increase).
#heterogeneity does not explain anything. 

```
Figure making of model 4: Lu_het, effort, MAP

```{r}
plot(Bee$lu_het, Bee$Eulaema_nigrita, las=1, col="grey", pch=16, ylab = "Abundance", xlab = "Land-use Heterogeneity")

NewLU = seq(min(Bee$lu_het), max(Bee$lu_het), length.out = 500)
newMAP = rep(mean(Bee$MAP), length(NewLU)) #keep the percipitation the same the whole time.
neweffort = rep(mean(Bee$effort), length(NewLU)) 
y_hat = predict(model4, newdata=list(MAP=newMAP, 
                                lu_het=NewLU, effort=neweffort), 
                type="response")
lines(NewLU, y_hat,lwd=2)


newMAP2 = rep(mean(Bee$MAP)+sd(Bee$MAP), length(NewLU))
y_hat2 = predict(model4, newdata=list(MAP=newMAP2, 
                                lu_het=NewLU, effort=neweffort), 
                type="response")
lines(NewLU, y_hat2, lwd=2, col=2)


newMAP3 = rep(mean(Bee$MAP)-sd(Bee$MAP), length(NewLU))
y_hat3 = predict(model4, newdata=list(MAP=newMAP3, 
                                lu_het=NewLU, effort=neweffort), 
                type="response")
lines(NewLU, y_hat3, lwd=2, col=3)
legend("topleft", lty=1, lwd=2, col=1:3, bty="n", 
       legend=c("MAP = Mean",
                "MAP = Mean + SD",
                "MAP = Mean - SD"))
```

In biology we often have count data with many zeros. This is one of the causes of overdispersion as discussed
above. Another way to deal with this is to split the analysis into two independent components, where the
first models the zeros, and the second models the counts given that at least 1 entity was observed. This is
called a hurdle model, where hurdle refers to the separation of zeros from non-zeros. We can fit a hurdle
model in two parts and then combine the predictions.
Below we define two new response variables, where the first (y1) is a 0/1 variable, and the second (y2) has
all the zeros set to NA

```{r}
y1 = ((y>1)*1)
m1 = glm(y1~x, family="binomial" (link="logit"))
y2 = y
y2[which(y==0)] = NA
m2 = glm(y2~x, family="poisson", na=na.exclude)
coefs1 = summary(m1)$coef
coefs2 = summary(m2)$coef
y_hat1 = coefs1[1,1] + coefs1[2,1]*x
y_hat2 = coefs2[1,1] + coefs2[2,1]*x
y_pred = invlogit(y_hat1)*exp(y_hat2)
par(mfrow=c(1,3))
plot(x, invlogit(y_hat1), las=1)
plot(x, exp(y_hat2), las=1)
plot(x, y_pred, las=1)
```

